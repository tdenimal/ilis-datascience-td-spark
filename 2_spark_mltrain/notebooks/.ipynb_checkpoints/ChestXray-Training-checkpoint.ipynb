{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook to train a model to diagnose thoracic pathology from chest X-rays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this Jupyter notebook is to demonstrate how we can build a AI-based Radiologist system using Apache Spark and Analytics Zoo to detect pneumonia and other diseases from chest x-ray images. The X-rays are made available by the United Statesâ€™ National Institutes of Health (NIH). The dataset contains over 120,000 images of frontal chest x-rays, each potentially labeled with one or more of fourteen different thoracic pathologies. We show how to build a multi-label image classification model in a distributed Apache Spark infrastructure, and demonstrate how to build complex image transformations and deep learning pipelines using Analytics Zoo with scalability and ease of use.\n",
    "\n",
    "For instructions on prerequisites for this notebook, refer to the GitHub readme."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the required packages\n",
    "The following modules are for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "# Ignoring the warnings to improve readability of the notebook\n",
    "warnings.filterwarnings(\"ignore\", message=\"numpy.dtype size changed\")\n",
    "\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "from math import ceil\n",
    "#from bigdl.dllib.optim.optimizer import SGD, SequentialSchedule, Warmup, Poly,\\Plateau, EveryEpoch, \n",
    "#TrainSummary,\\ValidationSummary, SeveralIteration, Step\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "from bigdl.dllib.nncontext import *\n",
    "from bigdl.dllib.feature.image.imagePreprocessing import *\n",
    "from bigdl.dllib.feature.common import ChainedPreprocessing\n",
    "from bigdl.dllib.keras.layers import Input, Flatten, Dense, GlobalAveragePooling2D, Dropout\n",
    "from bigdl.dllib.keras.metrics import AUC\n",
    "from bigdl.dllib.keras.optimizers import Adam\n",
    "from bigdl.dllib.keras.models import Model\n",
    "from bigdl.dllib.net.net_load import Net\n",
    "from bigdl.dllib.nnframes import NNEstimator, NNImageReader\n",
    "from bigdl.dllib.keras.objectives import BinaryCrossEntropy\n",
    "from pyspark.sql.types import StringType, ArrayType\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer learning and loading pre-trained models\n",
    "We use transfer learning for training the model. In the following cell, we show how to load a pre-trained Inception, ResNet-50, VGG, and a DenseNet model. These models are pre-trained with ImageNet dataset and are available [here](https://analytics-zoo.github.io/0.4.0/#ProgrammingGuide/image-classification/). Only one of the models is used in the actual training. You can switch between the different models below by calling the appropritate function to  see how they perform.\n",
    "\n",
    "*get_resent_model* function below is used to load an __ResNet-50__ Model. The function accepts two parameters:\n",
    "- *model_path* - This is the path in your HDFS where the model pretrained model is located\n",
    "- *label_length* - This is the number of labels for a given task. For this exercise, the Xrays can have 14 diseases. *label-length* is always 14.\n",
    "\n",
    "The function does the following:\n",
    "-  *Net.load_bigdl()* - loads a BigDL model. _Net_ package can be used to load models from other frameworks like Caffe, Torch and TensorFlow. This returns a _Model_.\n",
    "- *new_graph()* removes layers after \"pool5\"\n",
    "- *Input()* creates a new layer for the Xray images. The images are resized to 224x224 and have three channels\n",
    "- The input layer is added to the model using *to_keras*\n",
    "- We then flatten the neural network, add dropout and apply regularization\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bigdl.dllib.keras.layers import *\n",
    "\n",
    "# Function to load a ResNet50 model\n",
    "def build_model(label_length):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, 3, 3, input_shape=(3, 224, 224)))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=[2, 2]))\n",
    "\n",
    "    model.add(Conv2D(32, 3, 3))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=[2, 2]))\n",
    "\n",
    "    model.add(Conv2D(64, 3, 3))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=[2, 2]))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(64))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(label_length, activation=\"sigmoid\"))\n",
    "    #model.add(Activation(\"sigmoid\"))\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the AUC-ROC for a disease\n",
    "\n",
    "The following function calculates the ROC for disease *k*. We use ML Pipeline *BinaryClassificationEvaluator* for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_auc_for_kth_class(k, df, label_col=\"label\", prediction_col=\"prediction\"):\n",
    "    get_Kth = udf(lambda a: a[k], DoubleType())\n",
    "    extracted_df = df.withColumn(\"kth_label\", get_Kth(col(label_col))) \\\n",
    "        .withColumn(\"kth_prediction\", get_Kth(col(prediction_col))) \\\n",
    "        .select('kth_label', 'kth_prediction')\n",
    "    roc_score = BinaryClassificationEvaluator(rawPredictionCol='kth_prediction',\n",
    "                                              labelCol='kth_label', metricName=\"areaUnderROC\").evaluate(extracted_df)\n",
    "    return roc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the model and plot AUC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_and_plot(testDF):\n",
    "    predictionDF = nnModel.transform(testDF).persist(storageLevel=StorageLevel.DISK_ONLY)\n",
    "    label_texts= [\"Atelectasis\", \"Cardiomegaly\", \"Effusion\", \"Infiltration\", \"Mass\", \"Nodule\", \"Pneumonia\",\n",
    "                   \"Pneumothorax\", \"Consolidation\", \"Edema\", \"Emphysema\", \"Fibrosis\", \"Pleural_Thickening\", \"Hernia\"]\n",
    "    label_map = {k: v for v, k in enumerate(label_texts)}\n",
    "    chexnet_order = [\"Atelectasis\", \"Cardiomegaly\", \"Effusion\", \"Infiltration\", \"Mass\", \"Nodule\", \"Pneumonia\", \"Pneumothorax\", \"Consolidation\",\n",
    "     \"Edema\", \"Emphysema\", \"Fibrosis\", \"Pleural_Thickening\", \"Hernia\"]\n",
    "    total_auc = 0.0\n",
    "    roc_auc_label =dict()\n",
    "    for i in chexnet_order:\n",
    "        roc_score = get_auc_for_kth_class(label_map[i], predictionDF)\n",
    "        total_auc += roc_score\n",
    "        print('{:>12} {:>25} {:>5} {:<20}'.format('ROC score for ', i, ' is: ', roc_score))\n",
    "        roc_auc_label[i]=(roc_score)\n",
    "    print(\"Average AUC: \", total_auc / float(label_length))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main program\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-08 10:13:33,945 Thread-4 WARN The bufferSize is set to 4000 but bufferedIo is false: false\n",
      "2022-04-08 10:13:33,946 Thread-4 WARN The bufferSize is set to 4000 but bufferedIo is false: false\n",
      "2022-04-08 10:13:33,948 Thread-4 WARN The bufferSize is set to 4000 but bufferedIo is false: false\n",
      "2022-04-08 10:13:33,948 Thread-4 WARN The bufferSize is set to 4000 but bufferedIo is false: false\n",
      "22-04-08 10:13:33 [Thread-4] INFO  Engine$:121 - Auto detect executor number and executor cores number\n",
      "22-04-08 10:13:33 [Thread-4] INFO  Engine$:123 - Executor number is 1 and executor cores number is 6\n",
      "22-04-08 10:13:33 [Thread-4] INFO  ThreadPool$:95 - Set mkl threads to 1 on thread 15\n",
      "2022-04-08 10:13:33 WARN  SparkContext:69 - Using an existing SparkContext; some configuration may not take effect.\n",
      "22-04-08 10:13:33 [Thread-4] INFO  Engine$:446 - Find existing spark context. Checking the spark conf...\n",
      "Number of Executors = 0\n",
      "Number of Cores = 1\n",
      "Batch Size = 12\n"
     ]
    }
   ],
   "source": [
    "random.seed(1234)\n",
    "batch_size = 12 #1024 \n",
    "num_epoch = 15\n",
    "# \n",
    "#    model_path - Path for the pre-trained model file, data and the location to save the model after training. \n",
    "#                 The model path must match the function you are calling (ResNet-50, VGG or DenseNet)\n",
    "#    image_path - Path to all images\n",
    "#    label_path - Path to the label file (Data_Entry_2017.csv) available from NIH\n",
    "#    save_path = Path to save the model and intermediate results \n",
    "image_path = \"/opt/application/data/output\"\n",
    "label_path = \"/opt/application/data\"\n",
    "model_path = \"/opt/application/data/model\" \n",
    "\n",
    "# Get Spark Context\n",
    "sparkConf = create_spark_conf().setAppName(\"Chest X-ray Training\")\n",
    "sc = init_nncontext(sparkConf)\n",
    "spark = SparkSession.builder.config(conf=sparkConf).getOrCreate()\n",
    "\n",
    "# Make sure the batchsize is a multiple of (Number of executors * Number of cores)\n",
    "numexecutors = len(sc._jsc.sc().statusTracker().getExecutorInfos()) - 1\n",
    "numcores = int(sc.getConf().get('spark.executor.cores','1'))\n",
    "\n",
    "print(\"Number of Executors = \" +str(numexecutors))\n",
    "print(\"Number of Cores = \" + str(numcores))\n",
    "print(\"Batch Size = \" + str(batch_size))\n",
    "#if batch_size%(numexecutors*numcores)==0:\n",
    "#    print(\"Batchsize is a multiple of (Number of Executors * Number of cores. Good to proceed\")\n",
    "#else:\n",
    "#    print(\"Batchsize is NOT a multiple of (Number of Executors * Number of cores). Do not proceed !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load the data\n",
    "We then load the dataset. NIH has __[released](https://www.nih.gov/news-events/news-releases/nih-clinical-center-provides-one-largest-publicly-available-chest-x-ray-datasets-scientific-community)__ the chest xray has two sets (training and test). We have created a [notebook](ConvertXray-ConvertImages.ipynb) to read the Xray images from NIH an save them as training and test datasets (in two folders /trainingDF and /testDF). In the below code, we read these dataframes and combine them to a single Spark dataframe. We then sploy them to the actual training and validation Dataframes for our model. We use *ramdomSplit* to split the data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training images:  12\n",
      "Number of validation images:  12\n"
     ]
    }
   ],
   "source": [
    "label_length = 14\n",
    "label_texts = [\"Atelectasis\", \"Cardiomegaly\", \"Effusion\", \"Infiltration\", \"Mass\", \"Nodule\", \"Pneumonia\", \"Pneumothorax\",\n",
    "               \"Consolidation\", \"Edema\", \"Emphysema\", \"Fibrosis\", \"Pleural_Thickening\", \"Hernia\"]\n",
    "label_map = {k: v for v, k in enumerate(label_texts)}\n",
    "\n",
    "def text_to_label(text):\n",
    "    arr = [0.0] * len(label_texts)\n",
    "    for l in text.split(\"|\"):\n",
    "        if l != \"No Finding\":\n",
    "            arr[label_map[l]] = 1.0\n",
    "    return arr\n",
    "\n",
    "getLabel = udf(lambda x: text_to_label(x), ArrayType(DoubleType()))\n",
    "getName = udf(lambda row: os.path.basename(row[0]), StringType())\n",
    "imageDF = NNImageReader.readImages(image_path, sc, resizeH=256, resizeW=256, image_codec=1) \\\n",
    "    .withColumn(\"Image_Index\", getName(col('image')))\n",
    "\n",
    "labelDF = spark.read.load(label_path + \"/Data_Entry_2017_v2020.csv\", format=\"csv\", sep=\",\", inferSchema=\"true\", header=\"true\") \\\n",
    "        .select(\"Image_Index\", \"Finding_Labels\") \\\n",
    "        .withColumn(\"label\", getLabel(col('Finding_Labels'))) \\\n",
    "        #.withColumnRenamed('Image Index', 'Image_Index')\n",
    "\n",
    "totalDF = imageDF.join(labelDF, on=\"Image_Index\", how=\"inner\")\n",
    "#.withColumnRenamed(\"Finding Labels\", \"Finding_Labels\")\n",
    "\n",
    "#(trainingDF, validationDF) = totalDF.randomSplit([0.8, 0.2])\n",
    "trainingDF=totalDF\n",
    "validationDF=totalDF\n",
    "print(\"Number of training images: \", trainingDF.count())\n",
    "print(\"Number of validation images: \", validationDF.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the pre-trained model and optimiser\n",
    "We first load the pre-trained model. We use ResNet in the below example. It can be changed to any of the above defined models. We then load the optimiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating: createZooKerasSequential\n",
      "creating: createZooKerasConvolution2D\n",
      "creating: createZooKerasActivation\n",
      "creating: createZooKerasMaxPooling2D\n",
      "creating: createZooKerasConvolution2D\n",
      "creating: createZooKerasActivation\n",
      "creating: createZooKerasMaxPooling2D\n",
      "creating: createZooKerasConvolution2D\n",
      "creating: createZooKerasActivation\n",
      "creating: createZooKerasMaxPooling2D\n",
      "creating: createZooKerasFlatten\n",
      "creating: createZooKerasDense\n",
      "creating: createZooKerasActivation\n",
      "creating: createZooKerasDropout\n",
      "creating: createZooKerasDense\n"
     ]
    }
   ],
   "source": [
    "# Load the pretrained model\n",
    "#xray_model = get_resnet_model(model_path, label_length)\n",
    "xray_model = build_model(label_length)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image pre-processing\n",
    "\n",
    "We build *ChainedPreprocessing* to combine the following preprocessing.\n",
    "- *RowToImageFeature* - converts a Spark row to a BigDL ImageFeature\n",
    "- *ImageCenterCrop* - resizes the image to 224 x 224\n",
    "- *ImageHFlip* - Randomly flips 50% of the image horizontally\n",
    "- *ImageBrightness* - Randomly adjust the brigthness of 50% of the images\n",
    "- *ImageChannelNormalize* - Normalize the images by subtracting the mean value of the ImageNet images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating: createRowToImageFeature\n",
      "creating: createImageCenterCrop\n",
      "creating: createImageHFlip\n",
      "creating: createImageRandomPreprocessing\n",
      "creating: createImageBrightness\n",
      "creating: createImageRandomPreprocessing\n",
      "creating: createImageChannelNormalize\n",
      "creating: createImageMatToTensor\n",
      "creating: createImageFeatureToTensor\n",
      "creating: createChainedPreprocessing\n"
     ]
    }
   ],
   "source": [
    "transformer = ChainedPreprocessing(\n",
    "            [RowToImageFeature(), ImageCenterCrop(224, 224), ImageRandomPreprocessing(ImageHFlip(), 0.5),\n",
    "             ImageRandomPreprocessing(ImageBrightness(0.0, 32.0), 0.5),\n",
    "             ImageChannelNormalize(123.68, 116.779, 103.939), ImageMatToTensor(), ImageFeatureToTensor()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating: createZooKerasBinaryCrossEntropy\n",
      "creating: createSeqToTensor\n",
      "creating: createFeatureLabelPreprocessing\n",
      "creating: createNNEstimator\n",
      "creating: createEveryEpoch\n",
      "creating: createAUC\n",
      "creating: createAdam\n"
     ]
    }
   ],
   "source": [
    "classifier = NNEstimator(xray_model, BinaryCrossEntropy(), transformer) \\\n",
    "            .setBatchSize(batch_size) \\\n",
    "            .setMaxEpoch(num_epoch) \\\n",
    "            .setFeaturesCol(\"image\") \\\n",
    "            .setCachingSample(False) \\\n",
    "            .setValidation(EveryEpoch(), validationDF, [AUC()], batch_size).setOptimMethod(Adam())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22-04-08 10:14:11 [Thread-4] INFO  InternalDistriOptimizer$:987 - Sequential[c6293aa0] isTorch is false\n",
      "22-04-08 10:14:11 [Thread-4] INFO  DistriOptimizer$:826 - caching training rdd ...\n",
      "22-04-08 10:14:12 [Thread-4] INFO  DistriOptimizer$:652 - Cache thread models...\n",
      "22-04-08 10:14:12 [Executor task launch worker for task 0.0 in stage 145.0 (TID 97)] WARN  DistriOptimizer$:609 - Partitions of the training data are not evenlydistributed across the executors in the Spark cluster; are there sufficient trainingdata to be distributed?\n",
      "22-04-08 10:14:12 [Executor task launch worker for task 0.0 in stage 145.0 (TID 97)] INFO  ThreadPool$:95 - Set mkl threads to 1 on thread 768\n",
      "22-04-08 10:14:12 [Executor task launch worker for task 0.0 in stage 145.0 (TID 97)] INFO  ThreadPool$:95 - Set mkl threads to 1 on thread 768\n",
      "22-04-08 10:14:12 [Executor task launch worker for task 0.0 in stage 145.0 (TID 97)] INFO  ThreadPool$:95 - Set mkl threads to 1 on thread 768\n",
      "22-04-08 10:14:12 [Executor task launch worker for task 0.0 in stage 145.0 (TID 97)] INFO  ThreadPool$:95 - Set mkl threads to 1 on thread 768\n",
      "22-04-08 10:14:12 [Executor task launch worker for task 0.0 in stage 145.0 (TID 97)] INFO  ThreadPool$:95 - Set mkl threads to 1 on thread 768\n",
      "22-04-08 10:14:12 [Executor task launch worker for task 0.0 in stage 145.0 (TID 97)] INFO  ThreadPool$:95 - Set mkl threads to 1 on thread 768\n",
      "22-04-08 10:14:12 [Executor task launch worker for task 0.0 in stage 145.0 (TID 97)] INFO  ThreadPool$:95 - Set mkl threads to 1 on thread 768\n",
      "22-04-08 10:14:12 [Executor task launch worker for task 0.0 in stage 145.0 (TID 97)] INFO  DistriOptimizer$:635 - model thread pool size is 1\n",
      "2022-04-08 10:14:12 WARN  BlockManager:69 - Asked to remove block test_1weights0, which does not exist\n",
      "2022-04-08 10:14:12 WARN  BlockManager:69 - Asked to remove block test_1gradients0, which does not exist\n",
      "22-04-08 10:14:12 [Thread-4] INFO  DistriOptimizer$:654 - Cache thread models... done\n",
      "22-04-08 10:14:12 [Thread-4] INFO  DistriOptimizer$:164 - Count dataset\n",
      "22-04-08 10:14:12 [Thread-4] INFO  DistriOptimizer$:168 - Count dataset complete. Time elapsed: 0.0252827s\n",
      "22-04-08 10:14:12 [Thread-4] INFO  DistriOptimizer$:176 - config  {\n",
      "\tcomputeThresholdbatchSize: 100\n",
      "\tmaxDropPercentage: 0.0\n",
      "\twarmupIterationNum: 200\n",
      "\tisLayerwiseScaled: false\n",
      "\tdropPercentage: 0.0\n",
      " }\n",
      "22-04-08 10:14:12 [Thread-4] INFO  DistriOptimizer$:180 - Shuffle data\n",
      "22-04-08 10:14:12 [Thread-4] INFO  DistriOptimizer$:183 - Shuffle data complete. Takes 0.0019308s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22-04-08 10:14:13 [Thread-4] INFO  DistriOptimizer$:433 - [Epoch 1 12/12][Iteration 1][Wall Clock 0.7064477s] Trained 12.0 records in 0.7064477 seconds. Throughput is 16.986395 records/second. Loss is 8.839904. \n",
      "22-04-08 10:14:13 [Thread-4] INFO  DistriOptimizer$:475 - [Epoch 1 12/12][Iteration 1][Wall Clock 0.7064477s] Epoch finished. Wall clock time is 708.4581 ms\n",
      "22-04-08 10:14:13 [Thread-4] INFO  DistriOptimizer$:433 - [Epoch 2 12/12][Iteration 2][Wall Clock 1.2465411s] Trained 12.0 records in 0.538083 seconds. Throughput is 22.301392 records/second. Loss is 6.2162. \n",
      "22-04-08 10:14:13 [Thread-4] INFO  DistriOptimizer$:475 - [Epoch 2 12/12][Iteration 2][Wall Clock 1.2465411s] Epoch finished. Wall clock time is 1249.7118 ms\n",
      "22-04-08 10:14:13 [Thread-4] INFO  DistriOptimizer$:112 - [Epoch 2 12/12][Iteration 2][Wall Clock 1.2465411s] Validate model...\n",
      "22-04-08 10:14:14 [Thread-4] INFO  DistriOptimizer$:178 - [Epoch 2 12/12][Iteration 2][Wall Clock 1.2465411s] validate model throughput is 102.37396 records/second\n",
      "22-04-08 10:14:14 [Thread-4] INFO  DistriOptimizer$:181 - [Epoch 2 12/12][Iteration 2][Wall Clock 1.2465411s] AucScore is (Average score: 0.82142866, count: 12)\n",
      "score for each class is:\n",
      "0.99999994 \n",
      "0.5000001 \n",
      "0.5000004 \n",
      "0.5000004 \n",
      "0.99999994 \n",
      "0.99999994 \n",
      "0.99999994 \n",
      "0.99999994 \n",
      "0.99999994 \n",
      "0.99999994 \n",
      "0.5000004 \n",
      "0.99999994 \n",
      "0.99999994 \n",
      "0.49999994 \n",
      "\n",
      "22-04-08 10:14:14 [Thread-4] INFO  DistriOptimizer$:433 - [Epoch 3 12/12][Iteration 3][Wall Clock 1.9432841s] Trained 12.0 records in 0.6935723 seconds. Throughput is 17.30173 records/second. Loss is 4.443928. \n",
      "22-04-08 10:14:14 [Thread-4] INFO  DistriOptimizer$:475 - [Epoch 3 12/12][Iteration 3][Wall Clock 1.9432841s] Epoch finished. Wall clock time is 2065.5931 ms\n",
      "22-04-08 10:14:14 [Thread-4] INFO  DistriOptimizer$:112 - [Epoch 3 12/12][Iteration 3][Wall Clock 1.9432841s] Validate model...\n",
      "22-04-08 10:14:14 [Thread-4] INFO  DistriOptimizer$:178 - [Epoch 3 12/12][Iteration 3][Wall Clock 1.9432841s] validate model throughput is 109.95395 records/second\n",
      "22-04-08 10:14:14 [Thread-4] INFO  DistriOptimizer$:181 - [Epoch 3 12/12][Iteration 3][Wall Clock 1.9432841s] AucScore is (Average score: 0.82142866, count: 12)\n",
      "score for each class is:\n",
      "0.99999994 \n",
      "0.5000001 \n",
      "0.5000004 \n",
      "0.5000004 \n",
      "0.99999994 \n",
      "0.99999994 \n",
      "0.99999994 \n",
      "0.99999994 \n",
      "0.99999994 \n",
      "0.99999994 \n",
      "0.5000004 \n",
      "0.99999994 \n",
      "0.99999994 \n",
      "0.49999994 \n",
      "\n",
      "22-04-08 10:14:15 [Thread-4] INFO  DistriOptimizer$:433 - [Epoch 4 12/12][Iteration 4][Wall Clock 2.5930776s] Trained 12.0 records in 0.5274845 seconds. Throughput is 22.749485 records/second. Loss is 5.098582. \n",
      "22-04-08 10:14:15 [Thread-4] INFO  DistriOptimizer$:475 - [Epoch 4 12/12][Iteration 4][Wall Clock 2.5930776s] Epoch finished. Wall clock time is 2708.4617 ms\n",
      "22-04-08 10:14:15 [Thread-4] INFO  DistriOptimizer$:112 - [Epoch 4 12/12][Iteration 4][Wall Clock 2.5930776s] Validate model...\n",
      "22-04-08 10:14:15 [Thread-4] INFO  DistriOptimizer$:178 - [Epoch 4 12/12][Iteration 4][Wall Clock 2.5930776s] validate model throughput is 110.623856 records/second\n",
      "22-04-08 10:14:15 [Thread-4] INFO  DistriOptimizer$:181 - [Epoch 4 12/12][Iteration 4][Wall Clock 2.5930776s] AucScore is (Average score: 0.82142866, count: 12)\n",
      "score for each class is:\n",
      "0.99999994 \n",
      "0.5000001 \n",
      "0.5000004 \n",
      "0.5000004 \n",
      "0.99999994 \n",
      "0.99999994 \n",
      "0.99999994 \n",
      "0.99999994 \n",
      "0.99999994 \n",
      "0.99999994 \n",
      "0.5000004 \n",
      "0.99999994 \n",
      "0.99999994 \n",
      "0.49999994 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22-04-08 10:14:16 [Thread-4] INFO  DistriOptimizer$:433 - [Epoch 5 12/12][Iteration 5][Wall Clock 3.4271841s] Trained 12.0 records in 0.7187224 seconds. Throughput is 16.696293 records/second. Loss is 5.3179245. \n",
      "22-04-08 10:14:16 [Thread-4] INFO  DistriOptimizer$:475 - [Epoch 5 12/12][Iteration 5][Wall Clock 3.4271841s] Epoch finished. Wall clock time is 3542.6618 ms\n",
      "22-04-08 10:14:16 [Thread-4] INFO  DistriOptimizer$:112 - [Epoch 5 12/12][Iteration 5][Wall Clock 3.4271841s] Validate model...\n",
      "22-04-08 10:14:16 [Thread-4] INFO  DistriOptimizer$:178 - [Epoch 5 12/12][Iteration 5][Wall Clock 3.4271841s] validate model throughput is 104.78263 records/second\n",
      "22-04-08 10:14:16 [Thread-4] INFO  DistriOptimizer$:181 - [Epoch 5 12/12][Iteration 5][Wall Clock 3.4271841s] AucScore is (Average score: 0.82142866, count: 12)\n",
      "score for each class is:\n",
      "0.99999994 \n",
      "0.5000001 \n",
      "0.5000004 \n",
      "0.5000004 \n",
      "0.99999994 \n",
      "0.99999994 \n",
      "0.99999994 \n",
      "0.99999994 \n",
      "0.99999994 \n",
      "0.99999994 \n",
      "0.5000004 \n",
      "0.99999994 \n",
      "0.99999994 \n",
      "0.49999994 \n",
      "\n",
      "22-04-08 10:14:16 [Thread-4] INFO  DistriOptimizer$:433 - [Epoch 6 12/12][Iteration 6][Wall Clock 3.8091418s] Trained 12.0 records in 0.26648 seconds. Throughput is 45.03152 records/second. Loss is 6.085404. \n",
      "22-04-08 10:14:16 [Thread-4] INFO  DistriOptimizer$:475 - [Epoch 6 12/12][Iteration 6][Wall Clock 3.8091418s] Epoch finished. Wall clock time is 3928.8433 ms\n",
      "22-04-08 10:14:16 [Thread-4] INFO  DistriOptimizer$:112 - [Epoch 6 12/12][Iteration 6][Wall Clock 3.8091418s] Validate model...\n",
      "22-04-08 10:14:16 [Thread-4] INFO  DistriOptimizer$:178 - [Epoch 6 12/12][Iteration 6][Wall Clock 3.8091418s] validate model throughput is 112.71077 records/second\n",
      "22-04-08 10:14:16 [Thread-4] INFO  DistriOptimizer$:181 - [Epoch 6 12/12][Iteration 6][Wall Clock 3.8091418s] AucScore is (Average score: 0.82142866, count: 12)\n",
      "score for each class is:\n",
      "0.99999994 \n",
      "0.5000001 \n",
      "0.5000004 \n",
      "0.5000004 \n",
      "0.99999994 \n",
      "0.99999994 \n",
      "0.99999994 \n",
      "0.99999994 \n",
      "0.99999994 \n",
      "0.99999994 \n",
      "0.5000004 \n",
      "0.99999994 \n",
      "0.99999994 \n",
      "0.49999994 \n",
      "\n",
      "22-04-08 10:14:16 [Thread-4] INFO  DistriOptimizer$:433 - [Epoch 7 12/12][Iteration 7][Wall Clock 4.1825986s] Trained 12.0 records in 0.2537553 seconds. Throughput is 47.289654 records/second. Loss is 5.751213. \n",
      "22-04-08 10:14:16 [Thread-4] INFO  DistriOptimizer$:475 - [Epoch 7 12/12][Iteration 7][Wall Clock 4.1825986s] Epoch finished. Wall clock time is 4294.074 ms\n",
      "22-04-08 10:14:16 [Thread-4] INFO  DistriOptimizer$:112 - [Epoch 7 12/12][Iteration 7][Wall Clock 4.1825986s] Validate model...\n",
      "22-04-08 10:14:17 [Thread-4] INFO  DistriOptimizer$:178 - [Epoch 7 12/12][Iteration 7][Wall Clock 4.1825986s] validate model throughput is 101.37935 records/second\n",
      "22-04-08 10:14:17 [Thread-4] INFO  DistriOptimizer$:181 - [Epoch 7 12/12][Iteration 7][Wall Clock 4.1825986s] AucScore is (Average score: 0.82142866, count: 12)\n",
      "score for each class is:\n",
      "0.99999994 \n",
      "0.5000001 \n",
      "0.5000004 \n",
      "0.5000004 \n",
      "0.99999994 \n",
      "0.99999994 \n",
      "0.99999994 \n",
      "0.99999994 \n",
      "0.99999994 \n",
      "0.99999994 \n",
      "0.5000004 \n",
      "0.99999994 \n",
      "0.99999994 \n",
      "0.49999994 \n",
      "\n",
      "22-04-08 10:14:17 [Thread-4] INFO  DistriOptimizer$:433 - [Epoch 8 12/12][Iteration 8][Wall Clock 4.7820703s] Trained 12.0 records in 0.4879963 seconds. Throughput is 24.59035 records/second. Loss is 6.260075. \n",
      "22-04-08 10:14:17 [Thread-4] INFO  DistriOptimizer$:475 - [Epoch 8 12/12][Iteration 8][Wall Clock 4.7820703s] Epoch finished. Wall clock time is 4905.9771 ms\n",
      "22-04-08 10:14:17 [Thread-4] INFO  DistriOptimizer$:112 - [Epoch 8 12/12][Iteration 8][Wall Clock 4.7820703s] Validate model...\n",
      "22-04-08 10:14:17 [Thread-4] INFO  DistriOptimizer$:178 - [Epoch 8 12/12][Iteration 8][Wall Clock 4.7820703s] validate model throughput is 122.73327 records/second\n",
      "22-04-08 10:14:17 [Thread-4] INFO  DistriOptimizer$:181 - [Epoch 8 12/12][Iteration 8][Wall Clock 4.7820703s] AucScore is (Average score: 0.82142866, count: 12)\n",
      "score for each class is:\n",
      "0.99999994 \n",
      "0.5000001 \n",
      "0.5000004 \n",
      "0.5000004 \n",
      "0.99999994 \n",
      "0.99999994 \n",
      "0.99999994 \n",
      "0.99999994 \n",
      "0.99999994 \n",
      "0.99999994 \n",
      "0.5000004 \n",
      "0.99999994 \n",
      "0.99999994 \n",
      "0.49999994 \n",
      "\n",
      "22-04-08 10:14:17 [Thread-4] INFO  DistriOptimizer$:433 - [Epoch 9 12/12][Iteration 9][Wall Clock 5.1434656s] Trained 12.0 records in 0.2374885 seconds. Throughput is 50.528763 records/second. Loss is 6.578816. \n",
      "22-04-08 10:14:17 [Thread-4] INFO  DistriOptimizer$:475 - [Epoch 9 12/12][Iteration 9][Wall Clock 5.1434656s] Epoch finished. Wall clock time is 5246.8499 ms\n",
      "22-04-08 10:14:17 [Thread-4] INFO  DistriOptimizer$:112 - [Epoch 9 12/12][Iteration 9][Wall Clock 5.1434656s] Validate model...\n",
      "22-04-08 10:14:17 [Thread-4] INFO  DistriOptimizer$:178 - [Epoch 9 12/12][Iteration 9][Wall Clock 5.1434656s] validate model throughput is 109.28763 records/second\n",
      "22-04-08 10:14:17 [Thread-4] INFO  DistriOptimizer$:181 - [Epoch 9 12/12][Iteration 9][Wall Clock 5.1434656s] AucScore is (Average score: 0.82142866, count: 12)\n",
      "score for each class is:\n",
      "0.99999994 \n",
      "0.5000001 \n",
      "0.5000004 \n",
      "0.5000004 \n",
      "0.99999994 \n",
      "0.99999994 \n",
      "0.99999994 \n",
      "0.99999994 \n",
      "0.99999994 \n",
      "0.99999994 \n",
      "0.5000004 \n",
      "0.99999994 \n",
      "0.99999994 \n",
      "0.49999994 \n",
      "\n",
      "22-04-08 10:14:18 [Thread-4] INFO  DistriOptimizer$:433 - [Epoch 10 12/12][Iteration 10][Wall Clock 5.4890834s] Trained 12.0 records in 0.2422335 seconds. Throughput is 49.53898 records/second. Loss is 6.5511456. \n",
      "22-04-08 10:14:18 [Thread-4] INFO  DistriOptimizer$:475 - [Epoch 10 12/12][Iteration 10][Wall Clock 5.4890834s] Epoch finished. Wall clock time is 5604.0615 ms\n",
      "22-04-08 10:14:18 [Thread-4] INFO  DistriOptimizer$:112 - [Epoch 10 12/12][Iteration 10][Wall Clock 5.4890834s] Validate model...\n",
      "22-04-08 10:14:18 [Thread-4] INFO  DistriOptimizer$:178 - [Epoch 10 12/12][Iteration 10][Wall Clock 5.4890834s] validate model throughput is 129.77393 records/second\n",
      "22-04-08 10:14:18 [Thread-4] INFO  DistriOptimizer$:181 - [Epoch 10 12/12][Iteration 10][Wall Clock 5.4890834s] AucScore is (Average score: 0.82467544, count: 12)\n",
      "score for each class is:\n",
      "0.99999994 \n",
      "0.5000001 \n",
      "0.545455 \n",
      "0.5000004 \n",
      "0.99999994 \n",
      "0.99999994 \n",
      "0.99999994 \n",
      "0.99999994 \n",
      "0.99999994 \n",
      "0.99999994 \n",
      "0.5000004 \n",
      "0.99999994 \n",
      "0.99999994 \n",
      "0.49999994 \n",
      "\n",
      "22-04-08 10:14:18 [Thread-4] INFO  DistriOptimizer$:433 - [Epoch 11 12/12][Iteration 11][Wall Clock 5.8412275s] Trained 12.0 records in 0.237166 seconds. Throughput is 50.597473 records/second. Loss is 6.578815. \n",
      "22-04-08 10:14:18 [Thread-4] INFO  DistriOptimizer$:475 - [Epoch 11 12/12][Iteration 11][Wall Clock 5.8412275s] Epoch finished. Wall clock time is 5939.0511 ms\n",
      "22-04-08 10:14:18 [Thread-4] INFO  DistriOptimizer$:112 - [Epoch 11 12/12][Iteration 11][Wall Clock 5.8412275s] Validate model...\n",
      "22-04-08 10:14:18 [Thread-4] INFO  DistriOptimizer$:178 - [Epoch 11 12/12][Iteration 11][Wall Clock 5.8412275s] validate model throughput is 35.743835 records/second\n",
      "22-04-08 10:14:18 [Thread-4] INFO  DistriOptimizer$:181 - [Epoch 11 12/12][Iteration 11][Wall Clock 5.8412275s] AucScore is (Average score: 0.8214287, count: 12)\n",
      "score for each class is:\n",
      "0.99999994 \n",
      "0.5000001 \n",
      "0.5000004 \n",
      "0.5000004 \n",
      "0.99999994 \n",
      "0.99999994 \n",
      "0.99999994 \n",
      "0.99999994 \n",
      "0.99999994 \n",
      "0.99999994 \n",
      "0.5000004 \n",
      "1.0 \n",
      "0.99999994 \n",
      "0.49999994 \n",
      "\n",
      "22-04-08 10:14:19 [Thread-4] INFO  DistriOptimizer$:433 - [Epoch 12 12/12][Iteration 12][Wall Clock 6.5676095s] Trained 12.0 records in 0.6285584 seconds. Throughput is 19.091305 records/second. Loss is 6.2972274. \n",
      "22-04-08 10:14:19 [Thread-4] INFO  DistriOptimizer$:475 - [Epoch 12 12/12][Iteration 12][Wall Clock 6.5676095s] Epoch finished. Wall clock time is 6908.6588 ms\n",
      "22-04-08 10:14:19 [Thread-4] INFO  DistriOptimizer$:112 - [Epoch 12 12/12][Iteration 12][Wall Clock 6.5676095s] Validate model...\n",
      "22-04-08 10:14:19 [Thread-4] INFO  DistriOptimizer$:178 - [Epoch 12 12/12][Iteration 12][Wall Clock 6.5676095s] validate model throughput is 105.62785 records/second\n",
      "22-04-08 10:14:19 [Thread-4] INFO  DistriOptimizer$:181 - [Epoch 12 12/12][Iteration 12][Wall Clock 6.5676095s] AucScore is (Average score: 0.82142866, count: 12)\n",
      "score for each class is:\n",
      "0.99999994 \n",
      "0.5000001 \n",
      "0.5000004 \n",
      "0.5000004 \n",
      "0.99999994 \n",
      "0.99999994 \n",
      "0.99999994 \n",
      "0.99999994 \n",
      "0.99999994 \n",
      "0.99999994 \n",
      "0.5000004 \n",
      "0.99999994 \n",
      "0.99999994 \n",
      "0.49999994 \n",
      "\n",
      "22-04-08 10:14:19 [Thread-4] INFO  DistriOptimizer$:433 - [Epoch 13 12/12][Iteration 13][Wall Clock 7.139459s] Trained 12.0 records in 0.2308002 seconds. Throughput is 51.993023 records/second. Loss is 7.439379. \n",
      "22-04-08 10:14:19 [Thread-4] INFO  DistriOptimizer$:475 - [Epoch 13 12/12][Iteration 13][Wall Clock 7.139459s] Epoch finished. Wall clock time is 7258.0071 ms\n",
      "22-04-08 10:14:19 [Thread-4] INFO  DistriOptimizer$:112 - [Epoch 13 12/12][Iteration 13][Wall Clock 7.139459s] Validate model...\n",
      "22-04-08 10:14:19 [Thread-4] INFO  DistriOptimizer$:178 - [Epoch 13 12/12][Iteration 13][Wall Clock 7.139459s] validate model throughput is 125.43471 records/second\n",
      "22-04-08 10:14:19 [Thread-4] INFO  DistriOptimizer$:181 - [Epoch 13 12/12][Iteration 13][Wall Clock 7.139459s] AucScore is (Average score: 0.82142866, count: 12)\n",
      "score for each class is:\n",
      "0.99999994 \n",
      "0.5000001 \n",
      "0.5000004 \n",
      "0.5000004 \n",
      "0.99999994 \n",
      "0.99999994 \n",
      "0.99999994 \n",
      "0.99999994 \n",
      "0.99999994 \n",
      "0.99999994 \n",
      "0.5000004 \n",
      "0.99999994 \n",
      "0.99999994 \n",
      "0.49999994 \n",
      "\n",
      "22-04-08 10:14:20 [Thread-4] INFO  DistriOptimizer$:433 - [Epoch 14 12/12][Iteration 14][Wall Clock 7.5120387s] Trained 12.0 records in 0.2540316 seconds. Throughput is 47.238216 records/second. Loss is 7.160582. \n",
      "22-04-08 10:14:20 [Thread-4] INFO  DistriOptimizer$:475 - [Epoch 14 12/12][Iteration 14][Wall Clock 7.5120387s] Epoch finished. Wall clock time is 7612.4877 ms\n",
      "22-04-08 10:14:20 [Thread-4] INFO  DistriOptimizer$:112 - [Epoch 14 12/12][Iteration 14][Wall Clock 7.5120387s] Validate model...\n",
      "22-04-08 10:14:20 [Thread-4] INFO  DistriOptimizer$:178 - [Epoch 14 12/12][Iteration 14][Wall Clock 7.5120387s] validate model throughput is 116.85439 records/second\n",
      "22-04-08 10:14:20 [Thread-4] INFO  DistriOptimizer$:181 - [Epoch 14 12/12][Iteration 14][Wall Clock 7.5120387s] AucScore is (Average score: 0.82142866, count: 12)\n",
      "score for each class is:\n",
      "0.99999994 \n",
      "0.5000001 \n",
      "0.5000004 \n",
      "0.5000004 \n",
      "0.99999994 \n",
      "0.99999994 \n",
      "0.99999994 \n",
      "0.99999994 \n",
      "0.99999994 \n",
      "0.99999994 \n",
      "0.5000004 \n",
      "0.99999994 \n",
      "0.99999994 \n",
      "0.49999994 \n",
      "\n",
      "22-04-08 10:14:20 [Thread-4] INFO  DistriOptimizer$:433 - [Epoch 15 12/12][Iteration 15][Wall Clock 7.8500796s] Trained 12.0 records in 0.2375919 seconds. Throughput is 50.50677 records/second. Loss is 6.2498975. \n",
      "22-04-08 10:14:20 [Thread-4] INFO  DistriOptimizer$:475 - [Epoch 15 12/12][Iteration 15][Wall Clock 7.8500796s] Epoch finished. Wall clock time is 7958.277 ms\n",
      "22-04-08 10:14:20 [Thread-4] INFO  DistriOptimizer$:112 - [Epoch 15 12/12][Iteration 15][Wall Clock 7.8500796s] Validate model...\n",
      "22-04-08 10:14:20 [Thread-4] INFO  DistriOptimizer$:178 - [Epoch 15 12/12][Iteration 15][Wall Clock 7.8500796s] validate model throughput is 110.237785 records/second\n",
      "22-04-08 10:14:20 [Thread-4] INFO  DistriOptimizer$:181 - [Epoch 15 12/12][Iteration 15][Wall Clock 7.8500796s] AucScore is (Average score: 0.82142866, count: 12)\n",
      "score for each class is:\n",
      "0.99999994 \n",
      "0.5000001 \n",
      "0.5000004 \n",
      "0.5000004 \n",
      "0.99999994 \n",
      "0.99999994 \n",
      "0.99999994 \n",
      "0.99999994 \n",
      "0.99999994 \n",
      "0.99999994 \n",
      "0.5000004 \n",
      "0.99999994 \n",
      "0.99999994 \n",
      "0.49999994 \n",
      "\n",
      "creating: createToTuple\n",
      "creating: createChainedPreprocessing\n",
      "CPU times: user 43.9 ms, sys: 7.97 ms, total: 51.8 ms\n",
      "Wall time: 11.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "nnModel = classifier.fit(trainingDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model and plot AUC accuracy for Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model on validation data:\n",
      "22-04-08 10:14:34 [Thread-4] INFO  NNModel:730 - Batch per thread: 2; Total number of cores: 6; Global batch size: 12\n",
      "22-04-08 10:14:35 [Executor task launch worker for task 0.0 in stage 257.0 (TID 149)] INFO  ThreadPool$:95 - Set mkl threads to 1 on thread 768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC score for                Atelectasis  is:  0.0                 \n",
      "ROC score for               Cardiomegaly  is:  0.5                 \n",
      "ROC score for                   Effusion  is:  0.5                 \n",
      "ROC score for               Infiltration  is:  0.5                 \n",
      "ROC score for                       Mass  is:  0.0                 \n",
      "ROC score for                     Nodule  is:  0.0                 \n",
      "ROC score for                  Pneumonia  is:  0.0                 \n",
      "ROC score for               Pneumothorax  is:  0.0                 \n",
      "ROC score for              Consolidation  is:  0.0                 \n",
      "ROC score for                      Edema  is:  0.0                 \n",
      "ROC score for                  Emphysema  is:  0.5                 \n",
      "ROC score for                   Fibrosis  is:  0.0                 \n",
      "ROC score for         Pleural_Thickening  is:  0.0                 \n",
      "ROC score for                     Hernia  is:  0.5                 \n",
      "Average AUC:  0.17857142857142858\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluating the model on validation data:\")\n",
    "evaluate_and_plot(validationDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the model for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at:  /opt/application/data/model/xray_model_classif\n"
     ]
    }
   ],
   "source": [
    "save_path = model_path + '/xray_model_classif'\n",
    "nnModel.model.saveModel(save_path + \".bigdl\", save_path + \".bin\", True)\n",
    "print('Model saved at: ', save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
